# -*- coding: utf-8 -*-
"""pytorch_workfow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uE1_qAe2ckubzzvjpX-2cmqIOjNg0kMu

## PyTorch Workflow Fundamentals

doc: https://www.learnpytorch.io/01_pytorch_workflow/
"""



things_to_cover = {
1: "data (prepare and load)",
2: "\nbuild model",
3: "\nfitting the model (training)",
4: "\nmaking predictions and evaluating a model (inference)",
5: "\nsaving and loading a model",
6: "\nputting it all togther"}

import torch
from torch import nn
import matplotlib.pyplot as plt

# check Pytorch version
torch.__version__



"""# 1. Data (preparing and loading)

* Excel spreadsheet
* Images of any kind
* Videos
* DNA
* Text

Machine learning is a game of two parts:
1. Get data into a numerical representation
2. Build a model to learn patterns in that numerical representation

Let's create some *known* data using the linear regression formula

A linear regression formula to make a straight line wiht *known* **parameters**
"""

# create known parameters

weight = 0.7
bias = 0.3

# create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10]

len(X), len(y)



"""### Splitting data into training and test sets (one of the most important concepts in machine learning in general)"""

# create a training and test set with our data

train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

"""How to better visualize the data"""

import matplotlib.pyplot as plt

def plot_prediction(train_data=X_train,
                    train_labels=y_train,
                    test_data=X_test,
                    test_labels=y_test,
                    predictions=None):
  """Plots training data, test data and compares predictions"""
  plt.figure(figsize=(10, 7))
  #plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

  # plot test data in green
  plt.scatter(test_data, test_labels, c='green', s=4, label='Testing data')

  if predictions is not None:
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

  # show the legend
  plt.legend(prop={"size": 14});

plot_prediction();





"""# 2. Build model

doc for OOP: https://realpython.com/python3-object-oriented-programming/

what our model does:
* start with random values: weights and bias
* look at training data and adjust the random values to better respresent(or get closer to) the ideal values(the weights & bias values we used to create the data)

how does it do so?
Through two main algorithms:
1. Gradient decent
2. Backpropagation
"""

# create a linear regression model class
from torch import nn
class LinearRegressionModel(nn.Module):  # almost everything in PyTorch is inherited from nn.Module
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad=True,
                                            dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad=True,
                                         dtype=torch.float))
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias



"""# Pytorch model building essentials

* torch.nn - contains all of the building for computational graphs (nn can be considered a computational graph)
* torch.nn.Parameter - what parameters should our model try and learn, often a pytorch layer from torch.nn will set these for us
* torch.nn.Module - is the base class for all neural network modules, if you should subclass it, you should overwrite forward()
* torch.optim - this is where the optimizers in PyTorch live, they will help with gradient descent
* def forward() - All nn.module subclasses required you to overwrite forward(), this method defines what happens in the forward computation

"""

torch.manual_seed(42)
torch.randn(1)



"""## checking the contents of our pytorch models


we have created a model, let's see what's inside

So we can check out our model parameters what's inside our models, using .parameters()
"""

# create a random seed

torch.manual_seed(42)

# create an instance of the model we created (this is the subclass of nn.Module)

model_0 = LinearRegressionModel()
#check out the parameters

list(model_0.parameters())

# List named parameters
model_0.state_dict()

weight, bias

"""## Making predictions using torch.inference_mode()
to check our model's predictive power, let's see how well it predicts y_test based on x_test
when we pass through our model, it's going to run it through the forward() method
"""

X_test, y_test

y_preds = model_0(X_test)
y_preds

# make predictions with model

with torch.inference_mode():
  y_preds = model_0(X_test)

y_preds

y_test



plot_prediction(predictions=y_preds)

"""## train model

* The whole idea of training is for a model to move from unknown random parameters to some **some** known parameters

or in other words from a poor representation of the data to a better representation of the data

* One way to measure how poor or how wrong your model's predictions are, is to use a loss function

Note: loss function may also be called cost function or criterion in different areas. For our case, it's refered to as loss function

Things we need:

* **Loss funciton** is a function to measure how wrong your model's predictions are to the ideal output, lower the better

* **Optimizer** takes into account the loss of a model and adjust the model's parameters (weights and bias) to improve the loss function.
    * Inside the optimizer you'll often have to set two parameters:
      * params = the model parameters you'd like to optimize, for eg., params=model_0.parameters()
      * lr (learning rate) - the learning rate os a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small lr results in small changes, lr results in large changes)

And specifically for pytorch, we need:
* a training loop
* a testing loop


"""



list(model_0.parameters())

# check out the model's parameters ( a parameters is a value that the model sets itself)
model_0.state_dict()

"""optimizers doc: https://pytorch.org/docs/stable/optim.html"""

# Setup a loss function
loss_fn = nn.L1Loss()

# Setup an optimizer (stochastic gradient descent)
optimizer  = torch.optim.SGD(params=model_0.parameters(),
                             lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set

loss_fn

"""**Q**: Which loss function and optimizer should I use?

**A**: This will be problem specifi. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.

For eg., for a regressioon model, a loss function of nn.L1Loss() and an optimizer like torch.optim.SGD() will suffice.

But for clarification problem like classfiying whether a photo is of a dog or a cat, you'll likely use a loss function of nn.BCELoss() (binary cross entropy loss).


"""



"""## building a training loop (and testing loop) in pytorch

A couple of things we need for a training loop:
0. Loop through the data

1. forward pass (this involves data moving through our model's forward() function) to make predictions on data - also called a forward propagation

2. Calculate the loss (compare the forward pass predictions to ground truth labels)

3. Optimize zero grad

4. Loss backwards - move backwards through the network to calculate the gradients of each of the parameters of our model with repsect to the loss (**back propogation**)

5. Optimizer step - use the optimizer to adjust the model parameters to try to improve the loss (**gradient decent**)


"""

torch.manual_seed(42)
# an epoch is one loop through the data... (this is a hyperparameter becsause we set it ourselves)
epochs = 200

epoch_count = []
loss_values = []
test_loss_values = []


### Training

# 0. loop through the data
for epoch in range(epochs):
  # set the mode to training mode
  model_0.train() # train mode in pytorch sets all parameters that require gradients to required gradients

  # 1. Forward pass
  y_pred = model_0(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Perform back propogation on the loss with respect to the parameters of the model
  loss.backward()

  # 5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default how the optimizer changes will accumulate through the loop so.. we have to zero them above in step 3 for the next iteration of the loop

  ## testing
  model_0.state_dict()

  ## testing
  model_0.eval() # turns off different settings in the model not needed for evalutation/testing
  with torch.inference_mode(): # this turns off gradinet tracking & a couple of more things behind the scenes

    # 1. Do the forward pass
    test_pred = model_0(X_test)

    # 2. Calculate the loss
    test_loss = loss_fn(test_pred, y_test)

  # print out what's happening
  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_values.append(loss.item())
    test_loss_values.append(test_loss.item())
    print(f"Epoch: {epoch}| Loss: {loss} | Test loss: {test_loss}")
    # print out model state_dict()
    print(model_0.state_dict())

with torch.inference_mode():
  y_preds_new = model_0(X_test)

import numpy as np

np.array(torch.tensor(loss_values).cpu().numpy()), test_loss_values

import matplotlib.pyplot as plt

plt.plot(epoch_count, loss_values, label='Train loss')
plt.plot(epoch_count, test_loss_values, label='Test loss')
plt.title("Training and Test Loss Curves")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()
plt.show()

print(len(epoch_count), len(loss_values), len(test_loss_values))

plot_prediction(predictions=y_preds_new);

print(f"Epoch: {epoch}| Loss: {loss} | Test loss: {test_loss}")



